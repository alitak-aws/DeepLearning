{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Deep Learning Training and HPO Code in a Local Sagemaker Instance for Dockerizing\n",
    "\n",
    "This notebook shows an example of a running Bayesian HPO and also training for a regression deep neural network written in Keras with a Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install hyperopt==0.2.4 Flask==1.1.2 seaborn -q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pickle import dump\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "print(raw_dataset.head())\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_dataset.to_csv('data/hp_train.csv', index=False)\n",
    "test_dataset.drop('Horsepower', axis=1).to_csv('data/hp_test.csv', index=False)\n",
    "test_dataset['Horsepower'].to_csv('data/true_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Function for preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(train_data):   \n",
    "\n",
    "    if not final_training:\n",
    "        skip = int(100/int(used_data_percentage))\n",
    "        train_data = train_data[::skip]\n",
    "\n",
    "    train_data = train_data.dropna()\n",
    "    print(train_data.columns)\n",
    "\n",
    "    train_data = train_data.astype('float32')\n",
    "    \n",
    "    train_x = train_data.drop([target], axis=1)\n",
    "    train_y = train_data[target]\n",
    "\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = train_validation_split)\n",
    "    \n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(train_x)\n",
    "#     scaler = preprocessing.MinMaxScaler().fit(train_x)\n",
    "    dump(scaler, open(os.path.join(model_path, 'scaler.pkl'), 'wb'))\n",
    "    \n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "    \n",
    "    print(f'train_x Max: {train_x.max()}, {train_x.min()}')\n",
    "\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "df = pd.read_csv('data/df_train.csv')\n",
    "df = df.drop('Target', axis=1).copy()\n",
    "df['Target'] = df.iloc[:, 0] + df.iloc[:, 1] + df.iloc[:, 2]\n",
    "\n",
    "sns.set(rc={'figure.figsize':(16,16)})\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Function for doing a final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(params):\n",
    "#     input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "#     if len(input_files) == 0:\n",
    "#         raise ValueError(('There are no files in {}.\\n' +\n",
    "#                           'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "#                           'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "#                           'does not have permission to access the data.').format(training_path, channel_name))\n",
    "#     raw_data = [ pd.read_csv(file) for file in input_files if file.endswith('.csv')]\n",
    "    raw_data = pd.read_csv('data/hp_train.csv')\n",
    "    train_x, train_y, test_x, test_y = data_prep(raw_data)\n",
    "    print('data loaded')    \n",
    "    start = timer()\n",
    "  \n",
    "    #######################################################\n",
    "    model = Sequential()\n",
    "    for i in range(params['num_dense_layers']-1):\n",
    "        if i ==0:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_1'], kernel_initializer='normal',input_dim = train_x.shape[1], activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "        else:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_'+str(i+1)], kernel_initializer='normal', activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal')) #,activation=params['last_activation_f']))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    model.compile(loss=loss_metric, optimizer = params['optimizer'], metrics=[loss_metric])\n",
    "    model.summary()\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor= monitor_metric, patience=early_stopping_patience, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor= monitor_metric, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor= monitor_metric, factor=0.1, patience=lr_update_patience, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "    \n",
    "    print(f'Train Max: {train_x.max()}, {train_x.min()}, Target: {train_y.max()}, {train_y.min()}')\n",
    "    print('***********')\n",
    "    \n",
    "    history = model.fit(train_x, train_y,\n",
    "              callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "              epochs=params['nb_epochs'],\n",
    "              verbose=2,\n",
    "              validation_data=(test_x, test_y))\n",
    "\n",
    "    predictions=model.predict(test_x)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Actual','Predicted'])\n",
    "    df['Actual'] = test_y\n",
    "    df['Predicted'] = predictions\n",
    "    diff = abs(df['Actual']  - df['Predicted'])/df['Actual'] \n",
    "    q95 = diff.quantile(.95)\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(model_path, 'model.json'), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(model_path, 'model.h5'))\n",
    "    print(\"Saved model to disk\")\n",
    "    ###########\n",
    "    print('q95  {}'.format(q95))\n",
    "    run_time = timer() - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Function for doing Bayesian HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_q95 = 10e10\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    \n",
    "    input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(training_path, channel_name))\n",
    "    raw_data = [ pd.read_csv(file) for file in input_files if file.endswith('.csv')]\n",
    "    raw_data = pd.concat(raw_data)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = data_prep(raw_data)\n",
    "    print('data loaded')\n",
    "\n",
    "    global ITERATION\n",
    "    print('Iteration: {}'.format(ITERATION))\n",
    "    \n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "  \n",
    "    #######################################################\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(params['num_dense_layers']-1):\n",
    "        if i ==0:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_1'], kernel_initializer='normal',input_dim = train_x.shape[1], activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "        else:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_'+str(i+1)], kernel_initializer='normal', activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation= params['last_activation']))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    model.compile(loss=loss_metric, optimizer = params['optimizer'], metrics=[loss_metric])\n",
    "    #model.summary()\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor= monitor_metric, patience=early_stopping_patience, verbose=0, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor= monitor_metric, factor=0.1, patience=lr_update_patience, verbose=1, epsilon=1e-4, mode='min')\n",
    "    \n",
    "    \n",
    "    print(f'Train Max: {train_x.max()}, {train_x.min()}, Target: {train_y.max()}, {train_y.min()}')\n",
    "    print('***********')\n",
    "    \n",
    "    history = model.fit(train_x, train_y,\n",
    "              callbacks=[earlyStopping, reduce_lr_loss],\n",
    "              epochs=params['nb_epochs'],\n",
    "              verbose=2,\n",
    "              validation_data=(test_x, test_y))\n",
    "\n",
    "    predictions=model.predict(test_x)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Actual','Predicted'])\n",
    "    df['Actual'] = test_y\n",
    "    df['Predicted'] = predictions\n",
    "    diff = abs(df['Actual']  - df['Predicted'])/df['Actual'] \n",
    "    q95 = diff.quantile(.95)\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_q95\n",
    "    global short_model_summary\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if q95 < best_q95:\n",
    "        ###########\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(os.path.join(model_path, 'model.json'), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(os.path.join(model_path, 'model.h5'))\n",
    "        \n",
    "        stringlist = []\n",
    "        model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        short_model_summary = \"\".join(stringlist)\n",
    "\n",
    "        print(\"Saved model to disk\")\n",
    "        ###########\n",
    "        \n",
    "        # Update the regression accuracy.\n",
    "        best_q95 = q95\n",
    "    print(100*'=')\n",
    "    print(50*' ','      Iteration: \\n', ITERATION)\n",
    "    print('             q95:  \\n{}'.format(q95))\n",
    "    print('             best_q95:  \\n {}'.format(best_q95))\n",
    "    print(100*'=')\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    #######################################################    \n",
    "\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': q95,'params': params, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Define  HyperParameters for Training and HPO (equivalent of section 5-D in the train script)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define parameters for Final Training or HPO\n",
    "final_training = True  # This flag switched between Final Training mode (True) and HPO mode (False)\n",
    "\n",
    "if final_training: # If we are doing Final Training\n",
    "    final_training = True\n",
    "    target = 'Horsepower'\n",
    "    batch_normalization = False\n",
    "    include_dropout = False\n",
    "    dropout_f = .2\n",
    "    early_stopping_patience = 15\n",
    "    train_validation_split = .15\n",
    "    lr_update_patience = 7\n",
    "    loss_metric = 'mae'\n",
    "    monitor_metric = 'val_mean_absolute_error'\n",
    "    num_layers_f = 8\n",
    "    nodes =  [1024,64,1024,32,32,64,512] # The number of nodes (length of \"nodes\" list) should be num_layers_f-1 because the last layer has 1 node and is automatically added\n",
    "    nb_epochs_f = 10\n",
    "    batch_size_f = 32\n",
    "    optimizer_f = 'adam'\n",
    "    last_activation_f = 'tanh'\n",
    "       \n",
    "else:  # If we are doing HPO\n",
    "    final_training = False\n",
    "    target = 'Target'\n",
    "    batch_normalization = False\n",
    "    include_dropout = False\n",
    "    dropout = [.2,.3,.5]\n",
    "    early_stopping_patience = 15\n",
    "    lr_update_patience = 7\n",
    "    loss_metric = 'mae'\n",
    "    monitor_metric = 'val_mean_absolute_error'\n",
    "    used_data_percentage = 10\n",
    "    train_validation_split = .15\n",
    "    MAX_EVALS = 3\n",
    "    randstate = 50\n",
    "    num_layers_low = 1\n",
    "    num_layers_high = 9\n",
    "    choice_of_node_numbers = [16,32,64,128,256,512,1024,2048] # Here you can give the possible node size for layers. If you want to only have small number of nodes, remove the high values from this list. \n",
    "    nb_epochs = 3\n",
    "    batch_size = [32,64,128]\n",
    "    optimizer = ['adam']\n",
    "    last_activation = ['tanh']  # Activation for the layer with one node. Options for this are 'linear' and 'tanh'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Putting above parameters in dictionaries that can be used by Training or HPO functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_training:   # If we are doing Final Training\n",
    "    parameters = {   'num_dense_layers': num_layers_f,\n",
    "                'num_dense_nodes': {'num_dense_nodes_'+str(k+1): nodes[k] for k in range(num_layers_f-1)},\n",
    "                 'batch_size' : batch_size_f,\n",
    "                'nb_epochs' :  nb_epochs_f,\n",
    "                'dropout' :  dropout_f,\n",
    "                'optimizer': optimizer_f,\n",
    "                'last_activation_f': last_activation_f\n",
    "            }\n",
    "else:    # If we are doing HPO\n",
    "    space = {   'num_dense_layers': hp.choice('num_dense_layers', np.arange(num_layers_low, num_layers_high, dtype=int)),\n",
    "                'num_dense_nodes': {'num_dense_nodes_'+str(k+1): hp.choice('num_dense_nodes_'+str(k+1), choice_of_node_numbers) for k in range(num_layers_high)},\n",
    "                 'batch_size' : hp.choice('batch_size', batch_size),\n",
    "                'nb_epochs' :  nb_epochs,\n",
    "                'optimizer': hp.choice('optimizer',optimizer),\n",
    "                'last_activation': hp.choice('last_activation',last_activation)\n",
    "            }\n",
    "\n",
    "    if include_dropout:\n",
    "        space['dropout'] = hp.choice('dropout',dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- This is the main function which runs the final training or HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('Starting the training/HPO.')\n",
    "    try:\n",
    "        if final_training:\n",
    "            print('Starting the final training...')\n",
    "            train_final_model(parameters)\n",
    "    \n",
    "        else:\n",
    "            print('Starting the HPO...')\n",
    "            tpe_algorithm = tpe.suggest\n",
    "            bayes_trials = Trials()\n",
    "\n",
    "            # Global variable\n",
    "            global  ITERATION\n",
    "\n",
    "            ITERATION = 0\n",
    "            # Run optimization\n",
    "            best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "                        max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(randstate))\n",
    "\n",
    "\n",
    "            print('Training is complete.')\n",
    "            # Sort the trials with lowest loss (highest AUC) first\n",
    "            print(100*'=')\n",
    "            print('\\n                 Best Model:\\n')\n",
    "            bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "            \n",
    "            print('Model Summary: \\n\\n',short_model_summary)\n",
    "            print('\\n\\n\\n')\n",
    "            print(bayes_trials_results[0])\n",
    "            print('\\n\\n\\n')\n",
    "            print(100*'=')\n",
    "            \n",
    "            print('\\n                 2nd Best Model: \\n')\n",
    "            print(bayes_trials_results[1])\n",
    "            print(100*'=')\n",
    "\n",
    "            print('\\n                 3rd Best Model: \\n')\n",
    "            print(bayes_trials_results[2])\n",
    "            print(100*'=')\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failure\n",
    "        # Reason in the DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs\n",
    "        print(\n",
    "            'Exception during training: ' + str(e) + '\\n' + trc,\n",
    "            file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8- Define directories for data and model artifacts (equivalent of section 8-D in the train script) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'data'\n",
    "\n",
    "output_path = '../opt/ml/output' # You can create this outside of current directory.\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "model_path = '../opt/ml/model'# You can create this outside of current directory.\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9- Run train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training/HPO.\n",
      "Starting the final training...\n",
      "Index(['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
      "       'Acceleration', 'Model Year', 'Europe', 'Japan', 'USA'],\n",
      "      dtype='object')\n",
      "train_x Max: 2.755555524158868, -2.2222223792056597\n",
      "data loaded\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 1024)              10240     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1024)              66560     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 212,161\n",
      "Trainable params: 212,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train Max: 2.755555524158868, -2.2222223792056597, Target: 225.0, 46.0\n",
      "***********\n",
      "Train on 266 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 104.8642 - mae: 104.8642 - val_loss: 103.9959 - val_mae: 103.9959\n",
      "Epoch 2/10\n",
      " - 0s - loss: 102.4710 - mae: 102.4710 - val_loss: 93.7238 - val_mae: 93.7238\n",
      "Epoch 3/10\n",
      " - 0s - loss: 63.1911 - mae: 63.1911 - val_loss: 42.5809 - val_mae: 42.5809\n",
      "Epoch 4/10\n",
      " - 0s - loss: 35.8606 - mae: 35.8606 - val_loss: 33.0565 - val_mae: 33.0565\n",
      "Epoch 5/10\n",
      " - 0s - loss: 23.7621 - mae: 23.7621 - val_loss: 21.1436 - val_mae: 21.1436\n",
      "Epoch 6/10\n",
      " - 0s - loss: 18.8678 - mae: 18.8678 - val_loss: 16.4692 - val_mae: 16.4692\n",
      "Epoch 7/10\n",
      " - 0s - loss: 15.0180 - mae: 15.0180 - val_loss: 14.4682 - val_mae: 14.4682\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.7304 - mae: 12.7304 - val_loss: 13.7377 - val_mae: 13.7377\n",
      "Epoch 9/10\n",
      " - 0s - loss: 11.1696 - mae: 11.1696 - val_loss: 12.0849 - val_mae: 12.0849\n",
      "Epoch 10/10\n",
      " - 0s - loss: 10.7069 - mae: 10.7069 - val_loss: 12.2547 - val_mae: 12.2547\n",
      "Saved model to disk\n",
      "q95  0.26614039912819853\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10- Define functions for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the file that implements a flask server to do inferences. It's the\n",
    "# file that you will modify to implement the scoring for your own algorithm.\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "try:\n",
    "    from StringIO import StringIO ## for Python 2\n",
    "except ImportError:\n",
    "    from io import StringIO ## for Python 3\n",
    "    \n",
    "import flask\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "\n",
    "#############################\n",
    "from tensorflow import Graph, Session\n",
    "from keras import backend as K\n",
    "graph = Graph()\n",
    "\n",
    "#############################\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# prefix = '/opt/ml/'\n",
    "# model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "prefix = '../opt/ml/'\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "\n",
    "# A singleton for holding the model. This simply loads the model and holds it.\n",
    "# It has a predict function that does a prediction based on the model and the\n",
    "# input data.\n",
    "\n",
    "def loadmodel(weightFile, jsonFile):    \n",
    "    # load json and create model\n",
    "    json_file = open(jsonFile, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    reg = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    reg.load_weights(weightFile)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return reg\n",
    "   \n",
    "\n",
    "class ScoringService(object):\n",
    "    model = None                # Where we keep the model when it's loaded\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        \"\"\"\n",
    "        Get the model object for this instance,\n",
    "        loading it if it's not already loaded.\n",
    "        \"\"\"\n",
    "        if cls.model is None:\n",
    "            cls.model = loadmodel(os.path.join(model_path, 'model.h5'),os.path.join(model_path, 'model.json'))\n",
    "        return cls.model\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls,input):\n",
    "        \"\"\"For the input, do the predictions and return them.\n",
    "\n",
    "        Args:\n",
    "            input (a pandas dataframe): The data on which to do the\n",
    "            predictions.\n",
    "\n",
    "            There will be one prediction per row in the dataframe\n",
    "        \"\"\"\n",
    "        sess = K.get_session()\n",
    "        with sess.graph.as_default():\n",
    "            clf = cls.get_model()\n",
    "            return clf.predict(input)\n",
    "\n",
    "def transform_data(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.astype('float32')\n",
    "    scaler = load(open(os.path.join(model_path, 'scaler.pkl'), 'rb'))\n",
    "\n",
    "    # Feature Scaling\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "# # The flask app for serving predictions\n",
    "# app = flask.Flask(__name__)\n",
    "\n",
    "\n",
    "# @app.route('/ping', methods=['GET'])\n",
    "# def ping():\n",
    "#     \"\"\"\n",
    "#     Determine if the container is working and healthy.\n",
    "#     In this sample container, we declare it healthy if we can load the model\n",
    "#     successfully.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Health check -- You can insert a health check here\n",
    "#     health = ScoringService.get_model() is not None\n",
    "#     status = 200 if health else 404\n",
    "#     return flask.Response(\n",
    "#         response='\\n',\n",
    "#         status=status,\n",
    "#         mimetype='application/json')\n",
    "# @app.route('/invocations', methods=['POST'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11- Do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[188.37746 ],\n",
       "       [168.05074 ],\n",
       "       [158.99792 ],\n",
       "       [ 97.08253 ],\n",
       "       [108.06126 ],\n",
       "       [172.81042 ],\n",
       "       [152.35501 ],\n",
       "       [174.9477  ],\n",
       "       [105.071335],\n",
       "       [ 69.36477 ],\n",
       "       [ 65.50214 ],\n",
       "       [104.39715 ],\n",
       "       [144.93138 ],\n",
       "       [ 65.2666  ],\n",
       "       [153.64688 ],\n",
       "       [138.85928 ],\n",
       "       [144.7505  ],\n",
       "       [156.55649 ],\n",
       "       [ 95.862755],\n",
       "       [162.90477 ],\n",
       "       [183.1855  ],\n",
       "       [ 81.29385 ],\n",
       "       [ 96.86036 ],\n",
       "       [ 94.28782 ],\n",
       "       [ 64.965996],\n",
       "       [ 54.78132 ],\n",
       "       [104.968994],\n",
       "       [ 87.36685 ],\n",
       "       [ 98.02496 ],\n",
       "       [119.676895],\n",
       "       [ 70.150055],\n",
       "       [ 92.13894 ],\n",
       "       [ 90.480034],\n",
       "       [ 95.60489 ],\n",
       "       [ 56.021523],\n",
       "       [142.214   ],\n",
       "       [ 82.93515 ],\n",
       "       [ 79.85724 ],\n",
       "       [ 77.7049  ],\n",
       "       [ 91.62431 ],\n",
       "       [ 60.544167],\n",
       "       [ 68.70278 ],\n",
       "       [161.26259 ],\n",
       "       [ 52.324173],\n",
       "       [ 72.50601 ],\n",
       "       [ 59.08345 ],\n",
       "       [ 87.299706],\n",
       "       [ 85.84713 ],\n",
       "       [150.05865 ],\n",
       "       [ 85.472374],\n",
       "       [ 64.836365],\n",
       "       [128.50662 ],\n",
       "       [ 64.85269 ],\n",
       "       [152.52257 ],\n",
       "       [149.69766 ],\n",
       "       [151.67305 ],\n",
       "       [ 78.27302 ],\n",
       "       [ 65.44553 ],\n",
       "       [100.39513 ],\n",
       "       [ 75.75961 ],\n",
       "       [ 55.24076 ],\n",
       "       [ 63.62224 ],\n",
       "       [ 72.586464],\n",
       "       [111.06067 ],\n",
       "       [ 90.84036 ],\n",
       "       [ 58.1032  ],\n",
       "       [ 65.56816 ],\n",
       "       [ 73.37536 ],\n",
       "       [ 82.118   ],\n",
       "       [ 74.04818 ],\n",
       "       [101.93929 ],\n",
       "       [123.2792  ],\n",
       "       [ 77.99761 ],\n",
       "       [ 75.20187 ],\n",
       "       [ 74.28678 ],\n",
       "       [ 67.68081 ],\n",
       "       [ 72.24166 ],\n",
       "       [ 76.1616  ]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformation():\n",
    "    \"\"\"\n",
    "    Do an inference on a single batch of data. In this sample server, we take\n",
    "    data as CSV, convert it to a pandas data frame for internal use and then\n",
    "    convert the predictions back to CSV (which really just means one prediction\n",
    "    per line, since there's a single column.\n",
    "    \"\"\"\n",
    "    data = None\n",
    "\n",
    "    # Convert from CSV to pandas\n",
    "    s = 'data/hp_test.csv'   # MODIFIED\n",
    "    data = pd.read_csv(s, skiprows=1, header=None)\n",
    "    data = transform_data(data)\n",
    "    # Do the prediction\n",
    "    predictions = ScoringService.predict(data)\n",
    "\n",
    "#     # Convert from numpy back to CSV\n",
    "#     out = StringIO()\n",
    "#     pd.DataFrame(predictions).to_csv(out, header=False, index=False)\n",
    "#     result = out.getvalue()\n",
    "\n",
    "    return predictions   # MODIFIED\n",
    "#     return result, predictions\n",
    "\n",
    "\n",
    "transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12- Inference function in docker: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation():\n",
    "    \"\"\"\n",
    "    Do an inference on a single batch of data. In this sample server, we take\n",
    "    data as CSV, convert it to a pandas data frame for internal use and then\n",
    "    convert the predictions back to CSV (which really just means one prediction\n",
    "    per line, since there's a single column.\n",
    "    \"\"\"\n",
    "    data = None\n",
    "\n",
    "    # Convert from CSV to pandas\n",
    "    if flask.request.content_type == 'text/csv':\n",
    "        data = flask.request.data.decode('utf-8')\n",
    "        s = StringIO(data)\n",
    "        data = pd.read_csv(s, header=None)\n",
    "        data = transform_data(data)\n",
    "    else:\n",
    "        return flask.Response(response='This predictor only supports CSV data',status=415, mimetype='text/plain')\n",
    "\n",
    "    print('Invoked with {} records'.format(data.shape[0]))\n",
    "\n",
    "    # Do the prediction\n",
    "    predictions = ScoringService.predict(data)\n",
    "\n",
    "    # Convert from numpy back to CSV\n",
    "    out = StringIO()\n",
    "    pd.DataFrame(predictions).to_csv(out, header=False, index=False)\n",
    "    result = out.getvalue()\n",
    "\n",
    "    return flask.Response(response=result, status=200, mimetype='text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 1.15 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-1.15-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
